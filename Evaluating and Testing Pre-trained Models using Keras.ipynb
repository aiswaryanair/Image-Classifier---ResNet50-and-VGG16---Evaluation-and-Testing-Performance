{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item41\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 .VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under concrete_data_week4.zip\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week4.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q concrete_data_week4.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you unzip the data, you fill find the data has already been divided into a train, validation, and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "<a id=\"item42\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Import libraries, modules, and packages you will need. Make sure to import the *preprocess_input* function from <code>keras.applications.vgg16</code>.\n",
    "2. Use a batch size of 100 images for both training and validation.\n",
    "3. Construct an ImageDataGenerator for the training set and another one for the validation set. VGG16 was originally trained on 224 Ã— 224 images, so make sure to address that when defining the ImageDataGenerator instances.\n",
    "4. Create a sequential model using Keras. Add VGG16 model to it and dense layer.\n",
    "5. Compile the mode using the adam optimizer and the categorical_crossentropy loss function.\n",
    "6. Fit the model on the augmented data using the ImageDataGenerators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging the VGG16 model to build our classifier, so let's download it as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Define Global Constants\n",
    "\n",
    "We are obviously dealing with two classes, so num_classes is 2.\n",
    "The VGG16 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "We will training and validating the model using batches of 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "image_resize = 224\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct ImageDataGenerator Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator= ImageDataGenerator(preprocessing_function=preprocess_input,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the flow_from_directory method to get the training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator= data_generator.flow_from_directory('/concrete_data_week4/train',target_size=(image_resize,image_resize),\n",
    "                                                   batch_size=batch_size_training,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_generator= data_generator.flow_from_directory('/concrete_data_week4/valid',target_size=(image_resize,image_resize),\n",
    "                                                   batch_size=batch_size_validation,class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 13s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model= Sequential()\n",
    "model.add(VGG16(include_top=False,pooling='avg',weights='imagenet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.functional.Functional at 0x2ac859396d0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2ac83346ad0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.input_layer.InputLayer at 0x2ac843f1b90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac85778f90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86c5da90>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2ac86bf09d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86ba5b50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86bfbad0>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2ac84267690>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac94619f50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86bfbe90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac8435b210>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2ac83ce9290>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac9383ec90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86ba7f50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86bf0150>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2ac86be6bd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac859387d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac85938650>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2ac86c02e50>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2ac90ac8b90>,\n",
       " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x2ac83cedc90>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 1026 (4.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(train_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_33248\\3578781097.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "301/301 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9783 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 301 batches). You may need to use the repeat() function when building your dataset.\n",
      "301/301 [==============================] - 14866s 49s/step - loss: 0.0709 - accuracy: 0.9783 - val_loss: 0.0231 - val_accuracy: 0.9945\n",
      "Epoch 2/2\n",
      "301/301 [==============================] - 20652s 69s/step - loss: 0.0169 - accuracy: 0.9957\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs = num_epochs,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('/classifier_vgg16_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item43\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Evaluate Deep Learning Models using Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load your saved model that was built using the ResNet50 model. \n",
    "2. Construct an ImageDataGenerator for the test set. For this ImageDataGenerator instance, you only need to pass the directory of the test images, target size, and the **shuffle** parameter and set it to False.\n",
    "3. Use the **evaluate_generator** method to evaluate your models on the test data, by passing the above ImageDataGenerator as an argument. You can learn more about **evaluate_generator** [here](https://keras.io/models/sequential/).\n",
    "4. Print the performance of the classifier using the VGG16 pre-trained model.\n",
    "5. Print the performance of the classifier using the ResNet pre-trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cells to evaluate your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_ResNet= load_model(\"/classifier_resnet_model.h5\")\n",
    "model_ResNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 512)               14714688  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 1026      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14715714 (56.14 MB)\n",
      "Trainable params: 1026 (4.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_VGG16= load_model(\"/classifier_vgg16_model.h5\")\n",
    "model_VGG16.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Construct an ImageDataGenerator for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator= data_generator.flow_from_directory('/concrete_data_week4/test',target_size=(image_resize,image_resize),\n",
    "                                                   shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the evaluate_generator method to evaluate your models on the test data, by passing the above ImageDataGenerator as an argument.\n",
    " [evaluate_generator](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_33248\\2416416246.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  resnet50_accuracy = model_ResNet.evaluate_generator(test_generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.009541066363453865, 0.9980000257492065]\n"
     ]
    }
   ],
   "source": [
    "resnet50_accuracy = model_ResNet.evaluate_generator(test_generator)\n",
    "print(resnet50_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_33248\\1473358092.py:1: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  vgg16_accuracy = model_VGG16.evaluate_generator(test_generator)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.011607191525399685, 0.9959999918937683]\n"
     ]
    }
   ],
   "source": [
    "vgg16_accuracy = model_VGG16.evaluate_generator(test_generator)\n",
    "print(vgg16_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"item44\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Predict whether the images in the test data are images of cracked concrete or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the **predict_generator** method to predict the class of the images in the test data, by passing the test data ImageDataGenerator instance defined in the previous part as an argument. You can learn more about the **predict_generator** method [here](https://keras.io/models/sequential/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following cells to make your predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_33248\\99088406.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  Resnet_predictor=model_ResNet.predict_generator(test_generator)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Resnet_predictor=model_ResNet.predict_generator(test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_33248\\2493975669.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  vgg16_predictor = model_VGG16.predict_generator(test_generator)\n"
     ]
    }
   ],
   "source": [
    "vgg16_predictor = model_VGG16.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictResnet= [np.argmax(predictions) for predictions in Resnet_predictor]\n",
    "predictVGG16=[np.argmax(predictions) for predictions in vgg16_predictor]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'negative', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "pred_Resnet=[]\n",
    "for i in predictResnet:\n",
    "  if i==0:\n",
    "    pred_Resnet.append('negative')\n",
    "  else:\n",
    "    pred_Resnet.append('positive')\n",
    "print(pred_Resnet[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative', 'negative', 'negative', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "pred_VGG16=[]\n",
    "for i in predictVGG16:\n",
    "  if i==0:\n",
    "    pred_VGG16.append('negative')\n",
    "  else:\n",
    "    pred_VGG16.append('positive')\n",
    "print(pred_VGG16[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
