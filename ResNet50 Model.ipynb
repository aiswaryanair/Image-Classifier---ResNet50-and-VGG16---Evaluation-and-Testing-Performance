{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved under concrete_data_week3.zip\n"
     ]
    }
   ],
   "source": [
    "!python -m wget \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q concrete_data_week4.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    '/concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 40,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory('/concrete_data_week3/valid',\n",
    "                                                          target_size=(image_resize, image_resize),\n",
    "                                        batch_size=batch_size_validation,class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:979: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94765736/94765736 [==============================] - 10s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.functional.Functional at 0x2d6e04278d0>,\n",
       " <keras.src.layers.core.dense.Dense at 0x2d6c6638450>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.engine.input_layer.InputLayer at 0x2d6c6644610>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2d6d41f0110>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d3e80c50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6c6b38c10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5a69d90>,\n",
       " <keras.src.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2d6d24263d0>,\n",
       " <keras.src.layers.pooling.max_pooling2d.MaxPooling2D at 0x2d6d5a788d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d60455d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d1632ed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d41f2310>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d5a74f50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5aab310>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d41efe50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d60f3b10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d16388d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5a78910>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6123d50>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d41f2350>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d41f5590>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6142510>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d41f65d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5ab2150>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6141e90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d39e6e90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5a72f50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61499d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d605a050>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d39e6cd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d614b9d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6140e90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6143b90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6138f10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6120ad0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d613acd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6108050>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d163a590>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5a7b910>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d41f3950>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5a6be50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d41f0210>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d615f010>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d41f60d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6173890>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6122950>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6047990>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d615c1d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d617e8d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d59cbcd0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d1911390>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d6046fd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d383c8d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6191090>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6116bd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5aab3d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6193e90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6122c90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6115410>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61a9810>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d41f3f10>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d5ab3d10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6140a10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d16df1d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6058e10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d608b110>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61c9790>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5ab3d90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61cebd0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61cb950>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61cde50>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d61b1710>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61a90d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61a1fd0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61cb350>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d618c5d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6182010>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61a2d50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6172810>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6059b10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d41f1f10>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d6144510>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d610a810>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61de990>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d618f0d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61efe50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61f7510>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61f7a10>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6059c10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61de910>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61f4f90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5a6b910>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61f7010>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d6141290>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8006f10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8013b50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8013650>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61c9c10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d801bc50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8001410>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6138a10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d801ff50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d61cdb10>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d61477d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6144850>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6046f90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8012350>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8025d50>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8032990>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8032ed0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d801d410>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8006210>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d801d650>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d80024d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d801c9d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6059750>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80102d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61dd550>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6109110>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d617e690>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80328d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8047990>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d801bd50>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d61a2e90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80453d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d806a410>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d6088990>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d806b8d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8072c90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8073fd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80456d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d807ba50>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d617f3d0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d618f250>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d61ef350>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d808eb90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d808fd50>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d805d790>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8092ad0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d807a9d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8093c90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8027c10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80915d0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d808e7d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8025310>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d80ae050>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80a0e90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80a0390>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d808c6d0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8083090>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8071e10>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d80aef90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d807aad0>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80afa90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8068250>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d61a20d0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d6181f90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d61f6910>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d5ab3850>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d8033d90>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d80b6a90>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d809aa90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80b75d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d80cbf10>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80ad590>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d809bbd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d801d6d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d8011050>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d80dffd0>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d5aa8910>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6190510>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d8013010>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80db0d0>,\n",
       " <keras.src.layers.convolutional.conv2d.Conv2D at 0x2d6d6058510>,\n",
       " <keras.src.layers.normalization.batch_normalization.BatchNormalization at 0x2d6d808ead0>,\n",
       " <keras.src.layers.merging.add.Add at 0x2d6d80eee90>,\n",
       " <keras.src.layers.core.activation.Activation at 0x2d6d80b5050>,\n",
       " <keras.src.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x2d6d6190a10>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 2048)              23587712  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 4098      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23591810 (90.00 MB)\n",
      "Trainable params: 4098 (16.01 KB)\n",
      "Non-trainable params: 23587712 (89.98 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aiswa\\AppData\\Local\\Temp\\ipykernel_12544\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  fit_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "101/101 [==============================] - 9167s 91s/step - loss: 0.0682 - accuracy: 0.9756 - val_loss: 0.0198 - val_accuracy: 0.9944\n",
      "Epoch 2/2\n",
      "101/101 [==============================] - 2805s 28s/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.0126 - val_accuracy: 0.9964\n"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\AnacondaInstallation\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('/classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
